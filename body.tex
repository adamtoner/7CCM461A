\newpage
\part{The Model}

The Proportional Hazards Model is one of the most used statistical techniques in the world. It was introduced by Cox \fcite{cox72} with the aim of extending the results of Kaplan and Meier \fcite{kaplanmeier} "to the comparison of life tables and more generally to the incorporation of regression-like arguments into life-table analysis." Interestingly, these two papers are two of the most cited papers in statistics according to Ryan and Woodall \fcite{ryanwoodall}. So why is the Cox model so popular?

The Cox model is very attractive to statisticians because it is semi-parametric. What this effectively means is the hazard is divided into two parts:

\begin{itemize}
    \item a baseline hazard (which can be entirely arbitrary, nonparametric and time-dependent) which is common across all subjects, and
    \item a relative risk parameter which describes the effects of covariates on the hazard.
\end{itemize} 

This separation allows statisticians to build models based on the covariates without explicit or iterative calculation of the baseline hazard.

In this part of the thesis, I will describe the model in detail as proposed by Cox and explore surrounding methods and extensions. In section \ref{background}, I will give a history of the statistics used by Cox in the development of the model and the assumptions it is based upon. Section \ref{partial-likelihood} will consider the partial likelihood function as discussed in Cox \fcite{cox75}, why it can be consider it a full likelihood and why this is of benefit. Some methods of approximating and extending the original partial likelihood introduced by Cox \fcite{cox72} (which dealt solely with continuous time events) will be introduced briefly. Section \ref{baseline} will deal with the motives and methods for estimation of the baseline hazard.

\section{Background}\label{background}

The Cox model implements the hazard function in terms of a baseline hazard and a function of the covariates as follows

\begin{equation}\label{cox-hazard}
    \lambda(t;\z)=\lambda_0(t)e^{(\z\Beta)}.
\end{equation}

It is preferred over the logistic model for estimating survival time distributions as it considers the censored data and thus can obtain more information. The general approach to solving the model would first estimate the regression coefficients, then choose or estimate a suitable distribution for the baseline hazard, and finally estimate the full survivor and hazard functions.

The Cox model has two primary uses: variable impact estimation and survival time estimation. Of course, they impact one another but they are distinct.

Covariate impact can be estimated using model selection for the Cox model, this is the main focus of this thesis and is discussed in depth in part II. By estimating the regression coefficients to give the best model possible, the extent to which each covariate impacts the model is predicted. This hopefully allows for interpretable results which are useful in many fields for discovering: impactful genomes in cancer epidemiology, impact of material density on machine part failure, impact of age on loan defaulting, et cetera. 

Survival time or time-to-event estimation can be useful for predicting how long a patient is expected to live after a given diagnosis or the probability that a subject will default on their loan before repayment. The distributions underlying these regression estimations are of huge importance in medical, actuarial, business and engineering studies.

\subsection{Censoring}

I introduced censoring in section \ref{intro-censoring} and explained that most of the censoring that will be dealt with is right censoring. Here I would like to give a brief insight into how right censoring is usually dealt with and how it can affect the results of the model.

Obviously, it is beneficial to include as much information can be from the data when building the model. Thus in normal procedure, the individuals are included up until the point at which they are censored, then are removed from calculation at the end of the interval in which they are censored. 

Think of this as follows: if there is a set of $\{1,2,3,4\}$ of individuals and is is observed that 1 fails first, then 2 is censored, 3 then fails and finally 4 fails, the probability that at the time of first failure 1 fails as observed is given by $$pr(1\textrm{ fails}|\textrm{there is one failure from }\{1,2,3,4\})=\frac{pr(1)}{pr(1)+pr(2)+pr(3)+pr(4)}.$$ Individual 2 is then censored and the probability that 3 fails next is given by $$pr(3\textrm{ fails}|\textrm{there is one failure from }\{3,4\})=\frac{pr(3)}{pr(3)+pr(4)}.$$

In the commonly used methods, increased censoring will lead to an increase in bias in the model, i.e. in our example above, if subjects 2 and 4 were censored in the interval between failure of 1 and 3, the probability of 3 failing would be increased thus giving it more weight in the likelihood estimations introduced below in section \ref{partial-likelihood}.

Robins et al \fcite{robins-rotnitzky-92, robins-93, robins-finkelstein-00} discuss methods to deal censoring to produce consistent and asymptotically normal parameter estimation in the presence of censoring. The approach taken is to assume that given survival to a certain time $t_{(i)}$ and the covariate values $\z_{(i)}$, the censoring the in following interval is independent of survival time. This is referred to as  missing at random (MAR). However these methods are not commonly used and in the usual process as it is not always certain that the data is MAR.

Other possibilities are missing completely at random (MCAR) in which missingness / censoring occurs entirely at random, i.e. independently of the covariates and unknown parameters. Cox \fcite{cox72} stated this was an unsatisfactory assumption and should not be used. The last type of missing data which is relatively common is missing not at random (MNAR) in which data is assumed to be missing or censored due to some condition or a dependence on covariates. E.g. in a depression study where patients are more likely to stop checking in if they become more depressed.

\subsection{Proportional Hazards}

The assumption of proportional hazards is the basis of the model introduced by Cox \fcite{cox72}. If we have $n$ independent and identically distributed variables $T_i$ which have the hazard distribution defined by the Cox model, it is assumed that the hazard of one individual is proportional to the hazard of any other individual.

It can easily be seen that every hazard function is proportional to the baseline hazard under the assumption by rearranging \bref{cox-hazard} to give

\begin{equation}\label{prop-hazard}
    \frac{\lambda(t;\z)}{\lambda_0(t)}=\exp(\z\Beta).
\end{equation}

Clearly, the right side is not a function of $t$ and so the both sides are time invariant. The proportional hazards assumption holds if \bref{prop-hazard} holds. This condition can be restated as

\begin{equation}
    \Beta_\xi(t) = c\space \forall \xi\in\{1,\ldots,p\},
\end{equation}

i.e. the coefficients are time-invariant and have a scaling effect on the baseline hazard. Consider as well the hazard function for two subjects $j$ and $j'$ defined as in \bref{cox-hazard}. Dividing one by the other to find the proportion gives

\begin{equation}\label{hazard-ratio}
    \frac{\lambda(t;\z_j)}{\lambda(t;\z_{j'})} =\frac{\lambda_0(t)\exp(\z_j\Beta)}{\lambda_0(t)\exp(\z_{j'}\Beta)}=\frac{\exp(\z_j\Beta)}{\exp(\z_{j'}\Beta)}=e^{(\z_j-\z_{j'})\Beta},
\end{equation}

which is clearly time invariant on the right side and so must be time invariant on the left side as well. In layman's terms, this means if a subject 1 is twice as likely to have an event as subject 2, then this hazard ratio is constant, irrespective of time.

There are extensions of the Cox model discussed by Cox \fcite{cox72} to remove the proportional hazards assumption. These extensions allow for time-dependent covariates and time-dependent coefficients to be considered (see section \ref{extensions}). The extensions to the Cox model normally involve using splines or fractional polynomials, however it has been suggested in some model comparison discussions, see Patel et al \fcite{compphaft}, that more consideration be given to other hazard structures, namely the AFT. Rubio et al \fcite{rubioetal} explores the various hazard structures mentioned in the introduction and concludes that the AIC is a suitable method for selecting the hazard structure.

\subsubsection{Testing Proportional Hazards}

Equation \bref{hazard-ratio} is referred to as the hazard ratio and as mentioned is constant over time. We are able to test that all hazard ratios within the model are constant with time using both goodness-of-fit testing and graphical methods.

The essence of testing for proportional hazards boils down to examination of the coefficients. The null hypothesis states that the coefficients are time-invariant; should a test of the time-dependency of the coefficients give a statistically significant result that points to the rejection of the null hypothesis, the proportional hazards assumption has been broken and one of either an ad hoc modification, an extension to the model or a different model must be used.

There are many different graphical analysis methods to determine if the proportional hazards methods are likely to hold. Hess \fcite{hess95} suggests eight methods of graphical analysis, five of which are compared by Perrson \fcite{persson07} with the method proposed by Arjas \fcite{arjas88} in a simulation study using a Kolmogorov-Smirnov like maximum deviation criterion. Perrson \fcite{persson07} concludes that the Arjas plot is generally the preferred technique if the form of the hazard is anything but increasing, in which case either a smoothed plot of scaled Schoenfeld Residuals versus time or a smoothed plot of the ratio of log cumulative baseline hazard rates versus time are recommended.

Despite the advantages of the aforementioned tests, one of the most commonly used methods is using Kaplan-Meier plots. The data is stratified and the K-M curves are plotted. The curves should be proportional, i.e. they should not cross nor diverge.  

Graphical analysis can prove useful but it is limited in that it is difficult to define objective rules as to what would be deemed to reject the proportional assumptions hypothesis. Most of these methods also will not work well with continuous covariates as the graphs become too cluttered.

The most common methods used for testing proportional hazards in statistical software are residuals, including:

\begin{itemize}
\begin{spacing}{0}
    \item Cox-Snell Residuals
    \item Martingale Residuals
    \item Deviance Residuals
    \item Schoenfeld Residuals
    \item Scaled Schoenfeld Residuals
\end{spacing}
\end{itemize}

I will not discuss in more detail here how these residuals work; for details on how they are used within statistical software in relation to the Cox Model see Hintze \fcite{hintze2007user}.

It is worth mentioning the disadvantages of carrying out tests regarding proportional hazards. Of course it can be useful to check our assumptions hold, but we must now also consider the error implicit in the preliminary test carried out. It is possible that we get a false result for our appropriate model test. If we get a false positive, we could be using the Cox PH model when the hazards are indeed not proportional, or a false negative could lead to rejection of the Cox PH model when it accurately describes the data distribution.

\section{Partial Likelihood}\label{partial-likelihood}

A likelihood function is used to measure the goodness of fit of a statistical model based on a given set of data when the parameters of the model are unknown. It is a function of the parameters only and thus treats the random variables in the dataset as though they were fixed.

It is often the aim to maximise the likelihood function so that we find the parameters which will best fit the model. This parameter vector which maximises the likelihood function is referred to as the maximum likelihood estimate (MLE). When computing the MLE, it is often convenient to use the log-likelihood function to convert the difficult to work with products into more friendly summations.

The Bayesian and frequentist frameworks for statistics will be discussed in more detail in part II of this thesis, but it is worth mentioning now that the likelihood function plays a fundamental role in both. In Bayesian statistics, the maximum likelihood estimate is a special case of a maximum a posteriori estimation (MAP) in which the parameters follow a uniform prior distribution. From a frequentist standpoint, the MLE is a special case of an extremum estimator in which the objective function is the likelihood function.

Cox \fcite{cox72} introduced what he misleadingly referred to as a conditional likelihood which was a likelihood function for inference about $\Beta$ conditioned on the set ${t_i}$ of instants at which failure occurs. The logic here is no information about the hazard is contributed at any point other than those times at which failure occurs. 

\subsection{Continuous Time}

I will first consider the case of continuous time ($m_{(i)}=1\forall i$) as Cox \fcite{cox72} did. Extensions to discrete time will be discussed below. For each failure at time $t_i$, the probability that the failure which occurs is the one observed is 

\begin{equation}
    \frac{\lambda(t_{(i)}; \z_i)dt}{\sum_{j\in\R_i}\lambda(t_{(i)}; \z_{j})dt} = 
    \frac{\lambda_0(t_{(i)})e^{\z_i\Beta}}{\sum_{j\in\R_i}\lambda_0(t_{(i)})e^{\z_j\Beta}} = \frac{e^{\z_i\Beta}}{\sum_{j\in\R_i}e^{\z_j\Beta}}.
\end{equation}

This is the ratio of the hazard of the individual on which failure was observed over the sum of hazards across all the individuals who are at risk at time $t_i$. Notice that the baseline hazard cancels out leaving an equation which only varies with $\Beta$. The likelihood function is then:

\begin{equation}\label{partial-likelihood-eqn}
    L(\Beta;\x) = \prod_{i=1}^k \frac{e^{\z_i\Beta}}{\sum_{j\in\R_i}e^{\z_j\Beta}}
\end{equation}

The log-likelihood function for the model conditioned on ${t_i}$ is then given by taking the natural logarithm of the above function and summing over all events as follows:

\begin{align}
    l(\Beta;\x) &= \sum_{i=1}^k \log\bigg(\frac{e^{\z_i\Beta}}{\sum_{j\in\R_i}e^{\z_j\Beta}}\bigg)\\
    &= \sum_{i=1}^k \bigg( \log(e^{\z_i\Beta}) - \log(\sum_{j\in\R_i}e^{\z_j\Beta})\bigg)\\
    &= \sum_{i=1}^k \z_i\Beta - \sum_{i=1}^k \log(\sum_{j\in\R_i}e^{\z_j\Beta})
\end{align}

The above likelihood given by \bref{partial-likelihood-eqn} and the consequent log-likelihood are referred to as a partial likelihood. Cox \fcite{cox75} explained and discussed the partial likelihood. The principle of the partial likelihood is separating, as far as is possible, the incidental parameters and the nuisance parameters as shown by:

\begin{equation}
    L(\theta,\Beta;data) = L(\theta;data)\cdot L(\Beta;data)
\end{equation}

In the case of the Cox PH model, the coefficient vector, $\Beta$, is generally considered the incidental parameters and the nuisance parameter is the baseline hazard, $\lambda_0(t)$.

Consider the data as falling into two categories: right-censored and failure. Individuals observed to fail at time $t_i$ contribute to the density term $f(t_{(i)})$ and individuals censored at $t_i$ contribute to the survivor function $\F(t_{(i)})$. This gives the full likelihood function as

\begin{equation}
    L(\Beta,\lambda_0(\cdot);\x)=\prod_{i=1}^kf(t_{(i)};\Beta)^{\delta_i}\F(t_{(i)};\Beta)^{1-\delta_i}.
\end{equation}

The definition of the hazard function \bref{hazard-function} gives 

\begin{equation}
    L(\Beta,\lambda_0(\cdot);\x)=\prod_{i=1}^k\lambda(t_{(i)};\Beta)^{\delta_i}\F(t_{(i)};\Beta).    
\end{equation}

Substituting for the definition of the survivor function,

\begin{equation}
    \F(t;\Beta)=(\F_0(t))^{\exp(w\Beta)}=\big(e^{-\int_0^t\lambda_0(u)du}\big)^{\exp(w\Beta)},
\end{equation}

allows the likelihood to be written as

\begin{equation}
    L(\Beta,\lambda_0(\cdot);\x)=\prod_{i=1}^k\big(\lambda_0(t_{(i)})e^{w_i\Beta}\big)^{\delta_i}\big(e^{-\int_0^t\lambda_0(u)du}\big)^{\exp(w_i\Beta)}.
\end{equation}

Taking the log of the above likelihood will give the log-likelihood you first stated. Now multiplying by $\bigg(\frac{\sum_{j\in\mathscr{R}_i}\lambda_0(t_{(i)})\exp({w_j\Beta})}{\sum_{j\in\mathscr{R}_i}\lambda_0(t_{(i)})\exp({w_j\Beta})}\bigg)^{\delta_i}$ and cancelling the baseline hazards gives

\begin{equation}
    L(\Beta,\lambda_0(\cdot);\x)=\prod_{i=1}^k\bigg(\frac{e^{w_i\Beta}}{\sum_{j\in\mathscr{R}_i}e^{w_j\Beta}}\bigg)^{\delta_i}\prod_{i=1}^k\big(\sum_{j\in\mathscr{R}_i}\lambda_0(t_{(i)})e^{w_j\Beta}\big)^{\delta_i}\big(e^{-\int_0^t\lambda_0(u)du}\big)^{\exp(w_i\Beta)}.
\end{equation}

Cox \fcite{cox72} argued that the first term here provides most of the information about $\Beta$ and the last two terms contain most of the information about $\lambda_0(\cdot)$. With this in mind, the first term can be treated as a full likelihood when inference about $\Beta$ is the aim. Thus maximising \bref{partial-likelihood-eqn} gives the maximum partial likelihood estimate (MLPE).

\subsection{Discrete Time}\label{discrete-time}

Even considering just the partial likelihood and ignoring the likelihood of the nuisance parameter, problems still arise in discrete time when failure times are tied. Cox \fcite{cox72} states that a relatively ad hoc modification of the partial likelihood can be made. For a larger proportion of ties, a generalisation is required.

\subsubsection{Cox Method}

Cox \fcite{cox72} proposed a modification (now called the "discrete method") to the partial likelihood formula to account for tied failure times by considering the logistic model. The extension is given by

\begin{equation}
    L_{CX}(\Beta;\x)=\prod_{i=1}^k\frac{\exp(\s_{(i)}\Beta)}{\sum_{l\in\R(t_{(i)};m_{(i)})}\exp(\s_{(l)}\Beta)},
\end{equation}

where $\s_{(i)}=\sum_{j\in\D_{(i)}}\z_j$ and $\R(t_{(i)};m_{(i)})$ is the set of all subsets of size $m_{(i)}$ drawn from $\R(t_{(i)})$. The partial log-likelihood is then given by

\begin{equation}
    l(\Beta;\x)=\sum_{i=1}^k\s_{(i)}\Beta-\sum_{i=1}^k\log\Big(\sum_{l\in\R(t_{(i)};m_{(i)})}\exp(\s_{(l)}\Beta)\Big).
\end{equation}

This method assumes that if there are tied events, they truly happen at the same time. The problem with this method arises when there are a large number of ties which causes the denominator to become very complicated and computationally expensive.

\subsubsection{Kalbfeisch and Prentice}

Also referred to as the "exact method" of calculating the partial likelihood this method was introduced by Kalbfeisch and Prentice \fcite{kalbfleisch-prentice-73}. The method assumes that tied times occur due to imprecision in measurement and that there must be a true ordering of the events. The probability of each possible is ordering and summed to give the likelihood at time $t_{(i)}$ as

\begin{equation}
    L_{KP}(\Beta;\x)=\prod_{i=1}^k\frac{\exp(\s_{(i)}\Beta)}{\sum_{(p_1,\ldots,p_{m_{(i)}})\in Q_i}\prod_{j=1}^{m_{(i)}}\sum_{l\in\R(t_{(i)},p_j)}\exp(\z_l\Beta)},
\end{equation}

where $Q_i$ is the set of all permutations of indices of the individuals failing at $t_{(i)}$. $(p_1,\ldots,p_{m_{(i)}})$ is a single permutation in $Q_i$ and $\R(t_{(i)},p_r)$ is the set difference $\R(t_{(i)}-(p_1,\ldots,p_{r-1})$. This is easier to compute than the Cox method above, but it can still be computationally expensive.

\subsubsection{Breslow's Method}

The method proposed by Breslow \fcite{breslow74} generalises the results of Peto \fcite{peto-peto-72}. It is an approximation to the real likelihood function which replaces the denominator $\sum_{l\in\R(t_{(i)};m_{(i)})}\exp(\s_l\Beta)$ in Cox's method with $\big[\sum_{l\in\R(t_{(i)})}\exp(\z_l\Beta)\big]^{m_{(i)}}$ giving the partial likelihood

\begin{equation}
    L(\Beta;\x)\approx L_{BR}(\Beta;\x)=\prod_{i=1}^k\frac{\exp(\s_i\Beta)}{\big[\sum_{l\in\R(t_{(i)})}\exp(\z_l\Beta)\big]^{m_{(i)}}}.
\end{equation}

I will demonstrate the logic of this approximation with a simple example. Let two individuals 2 and 4 in the risk set $\R(t_{(i)})={1,2,3,4,5}$ fail at time $t_{(i)}$. The probability of observing 2 and 4 fail at $t_{(i)}$ is

\begin{gather*}
    \frac{e^{\z_2\Beta}}{e^{\z_1\Beta}+e^{\z_2\Beta}+e^{\z_3\Beta}+e^{\z_4\Beta}+e^{\z_5\Beta}}\cdot\frac{e^{\z_4\Beta}}{e^{\z_1\Beta}+e^{\z_3\Beta}+e^{\z_4\Beta}+e^{\z_5\Beta}}\\
    OR\\
    \frac{e^{\z_4\Beta}}{e^{\z_1\Beta}+e^{\z_2\Beta}+e^{\z_3\Beta}+e^{\z_4\Beta}+e^{\z_5\Beta}}\cdot\frac{e^{\z_2\Beta}}{e^{\z_1\Beta}+e^{\z_2\Beta}+e^{\z_3\Beta}+e^{\z_5\Beta}}.
\end{gather*}

This is approximately equal to $\frac{e^{(\z_2+\z_4)\Beta}}{\big[e^{\z_1\Beta}+e^{\z_2\Beta}+e^{\z_3\Beta}+e^{\z_4\Beta}+e^{\z_5\Beta}\big]^2}$. Clearly this approximation will not be accurate if $\frac{m_{(i)}}{r_{(i)}}$ is commonly large. Due to the addition of extra terms to the denominator whilst keeping the numerator consistent, the result will be biased toward zero. 

The log likelihood of this approximation is given by Kalbfleisch and Prentice \fcite{kalbfleisch-prentice-80} as

\begin{equation}
    l_{BR}(\Beta;\x)=\sum_{i=1}^{k}\s_{(i)}\Beta - \sum_{i=1}^km_{(i)}\log\Big(\sum_{l\in\R_i}\z_l\Beta\Big).
\end{equation}

\subsubsection{Efron's Method}

Although Breslow's approximation was the most common choice for years when it came to software packages due to its computational simplicity, Efron \fcite{efron-77} suggested another approximation which is usually more accurate and now the more popular choice. For a comparison of these estimators, see 

\begin{equation}
    L(\Beta;\x)\approx L_{EF}(\Beta;\x)=\prod_{i=1}^k\frac{\exp(\s_i\Beta)}{\prod_{j=1}^{m_{(i)}}\Big[\sum_{l\in\R_{(i)}}\exp(\z_l\Beta)-\frac{j-1}{m_{(i)}}\sum_{l\in\D_{(i)}}\exp(\z_l\Beta)\Big]}.
\end{equation}

Intuitively, this is similar to the Breslow approximation with a penalty term added to the denominator in an attempt to bring the approximation closer to the true value. However the approximation is still inaccurate unless the ratio of tied times to the risk sets is low for most times.

The Efron approximation is preferred for most cases today as it is more accurate than the Breslow approximation but still faster than the exact methods discussed above. The log partial likelihood is again given by Kalbfleisch and Prentice \fcite{kalbfleisch-prentice-80}:

\begin{equation}
    l_{EF}(\Beta;\x)=\sum_{i=1}^k\Bigg(\s_i\Beta - \sum_{j=1}^{m_{(i)}}\log\Big[\sum_{l\in\R_{(i)}}\exp(\z_l\Beta)-\frac{j-1}{m_{(i)}}\sum_{l\in\D_{(i)}}\exp(\z_l\Beta)\Big]\Bigg).
\end{equation}

\subsection{Finding the MPLE}

The above forms and approximations of the partial likelihood function with respect to $\Beta$ given the data can be "solved" to give the maximum partial likelihood estimate. This is usually done iteratively, as finding the parameter vector which maximises the likelihood explicitly is often far too complicated or even impossible. 

To find the MLPE, intuitively $\pdv{l(\Beta;\x)}{\Beta_\xi}=0$ for all $\xi\in\{1,\ldots,p\}$. Some of the commonly used iterative methods are

\begin{itemize}
\begin{spacing}{0}
    \item Newton-Raphson,
    \item Cyclic Coordinate Descent,
    \item Gradient Descent.
\end{spacing}
\end{itemize}

Other methods have been proposed and used over the years but I will not explain the methods further here. 
As a final note in this chapter, if there are few or no ties all of the mentioned likelihood forms will coincide.

\section{Baseline Hazard}\label{baseline}

As mentioned in section \ref{background}, the Cox model can be used for estimation of survival time. This requires the baseline hazard to be estimated and so it is not the primary use of the model. As mentioned, one of the key features of the Cox model is that is assumes nothing about the baseline hazard; it is allowed to take an entirely approximate form and the coefficient vector can still be estimated. Thus the most common ways of estimating survival time -- and by consequence the baseline hazard -- directly disregard an attractive feature of the model.

Nonetheless, the baseline hazard can be estimated in number of ways, both parametrically and nonparametrically. I will discuss both briefly here. 

\subsection{Nonparametric Estimation}

There are two main routes which are taken when estimating the baseline hazard nonparametrically. Kalbfeisch and Prentice \fcite{kalbfleisch-prentice-80} follow the same logic as the Kaplan-Meier estimate introduced in section \ref{non-parametric-estimators}; the hazard function is conditioned on the discrete times of events and is identically zero between the times. 

The baseline survivor function estimate can be written as

\begin{equation}
    \hat{\F}_{0,KP}(t)=\prod_{j;t_i<t}\hat{\pi}_j,
\end{equation}

where $\pi_i$ is the conditional probability of survival at time $t(i)$ for a subject at the baseline such that

\begin{equation}
    \pi_j = 1 - \lambda_0(t_j).
\end{equation}

To obtain the conditional probability for a subject with covariate vector $\z$, $\pi_i$ must be raised to the power of the hazard ratio. This leads to the likelihood function of $\F_{0,KP}$:

\begin{equation}\label{MLE-KP}
    L = \prod_{i=1}^k\bigg[\prod_{j\in\D_i}(1-\pi_i)^{\exp(\z_j\Beta)} \prod_{j\in\R_i-\D_i}\pi_i^{\exp(\z_j\Beta)}\bigg].
\end{equation}

Meier suggested maximising \bref{MLE-KP} with respect to $\Beta$ and $\pi_i$, however this can be quite computationally expensive. It is commonplace to get $\hat{\Beta}$ from other methods and then maximise the likelihood with respect to $\pi_i$ alone. The MLE can be obtained by solving

\begin{equation}\label{MLE-KP-max}
    \sum_{j\in\D_i} \frac{\exp(\z_i\hat{\Beta})}{1-\pi_i^{\exp(\z_i\hat{\Beta})}} = \sum_{j\in\R_i}\exp(\z_i\hat{\Beta})
\end{equation}

iteratively. A commonly suggested starting point for the iteration is

\begin{equation}
    \log\pi_i = -\frac{d_i}{\sum_{j\in\R_i}\exp(-\z_j\hat{\Beta})}.
\end{equation}

This is the Breslow estimate introduced below and should give a close starting point to the true solution. If there are no ties, the solution to \bref{MLE-KP-max} is given by

\begin{equation}
    \hat{\pi}_i = \Bigg(1-\frac{e^{\z_{j(i)}\hat{\Beta}}}{\sum_{j\in\R_i}e^{-\z_j\hat{\Beta}}}\Bigg)^{\exp(\z_{j(i)}\hat{\Beta})}.
\end{equation}

The other approach was taken proposed by Breslow in his discussion of Cox's paper\ucite{cox72} in which they extended the Nelson-Aalen estimate. Cox and Oakes \fcite{coxoakes1984} (chapter 7.8) describe this estimator in detail and I would encourage the reader to look there for further information if interested. The extensions allows the estimate given by \ref{nelson-aalen-estimator-eqn} to account for the covariates present in the semi-parametric model. Similarly to the Kalbfeisch and Prentice estimator, the estimate only assigns mass to the points at which times at which event times are recorded. The baseline cumulative hazard and survivor function estimates are then

\begin{equation}
    \hat{\Lambda}_{0,BR}(t)=\sum_{i;t_i<t}\frac{d_i}{\sum_{j\in R_i} \exp(
\z_j\Beta)},
\end{equation}

\begin{equation}
    \hat{\F}_{0,BR}(t)=\exp(-\hat{\Lambda}_{0,BR}(t))
\end{equation}

I will not give the derivation here but I will explain the logic. The denominator is the sum of relative risk factors or hazard ratios across all the subjects in the risk set at time $i$. Dividing the number of deaths at time $i$, $d_i$, by this summation gives the observed hazard at time $i$. The cumulative baseline hazard estimate is then calculated by summing the hazards up until the time of interest $t$. If there are no covariates (equivalently $\z$ has zero-dimension), the hazard ratio reduces to $1$ and the summation in the denominator counts the number of subjects in the risk set at time $i$. The estimator is then exactly \bref{nelson-aalen-estimator-eqn}.

In practice, the Breslow estimate is more commonly used, but it has the drawback that it will never reach zero even if the last observation is an event. The two estimators are compared by Xia et al \fcite{xia-et-al-18} who concluded that the K-P estimator results in a smaller MSE and less bias. 

\subsection{Parametric Estimation}

The distribution of the baseline hazard can be specified to give a parametric proportional hazards model. Because the development of the Cox model allows the baseline hazard to be entirely arbitrary, any reasonable distribution can be used. The distribution if specified is normally chosen based on assumption or prior knowledge so that it best fits the data. This does require the making of some assumptions which introduces bias, but can greatly increase the predictive power of the model.

The Weibull family of distributions is the only case in which the proportional hazards and the accelerated failure time models coincide. As a note, the exponential distribution is a special case of a Weibull distribution. Rubio et al \fcite{rubioetal} use the Exponentiated Weibull family of distributions in their paper studying general hazard structures. This is an extension of the Weibull family which introduces an extra shape parameter and can be used to describe hazard shapes of interest including: constant, bathtub, increasing, decreasing, and unimodal.

\section{Extensions}\label{extensions}

There are limitations to the Cox model which were mentioned when introducing the PH model above. To deal with cases outside of the assumptions, extensions have been proposed. 

A few of these extensions have already been discussed earlier in this thesis:

\begin{itemize}
    \item Specification of the baseline hazard -- the Cox PH model does not assume any particular form or features about the baseline hazard, so any specification of a parametric distribution of the underlying baseline hazard is technically an extension to the original model;
    \item Discrete time with an appreciable number of tied event times -- the methods discussed in section \ref{discrete-time} are extensions to the original model which assumed continuous time with no ties.
\end{itemize}

I will explain in great detail the extensions mentioned below, but they are worth a brief discussion so as to better define the limits of the model that is being used.

\subsection{Stratification}\label{stratification}

Splitting the data into predefined groups can be used to control for certain features of the data while examining the effects of the remaining covariates. 

The easiest way to stratify data is to introduce a categorical covariate $\w$ which has levels $\{1,\ldots,l\}$ for each category into which the data are sorted. It is possible to have multiple categorical covariates by the obvious extension to a categorical covariate vector.

The stratified hazard function for a given category is then given by an adaptation of \bref{cox-hazard} to

\begin{align}
    \lambda(t;\z,\w)&=\lambda_0(t)e^{\z\Beta + \w\bgamma}\\
    \label{condensed-stratified} &=\lambda_{\w}e^{\z\Beta}
\end{align}

where $\bgamma$ is the coefficient vector specifying the categories in which the individual lies. The covariate vectors, $\z$ and $\w$, and the coefficient vectors, $\Beta$ and $\bgamma$, could of course be combined, but it is generally easier to work with them separated so that there is a clear separation between variables which are desired to be controlled and those which are allowed to be free. This comes in useful in our condensation to \ref{condensed-stratified} in which the baseline hazard absorbs the effects of the categorical covariates so that the effects of the normal covariates can be found for a given category.

The utility here is found if it is expected that different categories of data will have different responses to the covariates. It allows for a model to be independently built for each category as opposed to a single model for the whole data set. This is a very common practice in fields such as medicine where it is believed that men and women will react differently to a given disease, e.g. breast cancer.

The hazard function for a particular category can be written as

\begin{equation}
    \lambda(t;\z,\w=\bv_j) = \lambda_{\bv_j}(t)e^{\z\Beta},
\end{equation}

where $\bv_j$ is the value of the categorical covariate vector, i.e. it specifies the categories for a given datum.

\subsection{Time-Varying Covariates}

It is a regular requirement to extend the Cox PH model to work with time-dependent covariates. E.g. a study of cancer patients with a covariate stating whether they have had surgery or not. The time-invariant PH model might record a positive result if the patient has ever received surgery, even near the end of the trial; as opposed to the time-dependent PH model which can describe the patient surgery status as negative up until receiving the surgery and positive thereafter. This provides more accurate information about the subject and in theory should help build a better model.

It is a fairly natural extension to the original Cox PH model to account for time-dependent covariates. The logic is, instead of evaluating the hazard for each subject once and using this repeatedly in the likelihood function, the hazard is calculated at each time, $t_{(i)}$, for which the subject is in the risk set $\R_i$.

The hazard function of the time-dependent PH model is given by

\begin{equation}
    \lambda(t;\z(t)=\lambda_0(t)e^{(\z(t)\Beta)},
\end{equation}

and the partial likelihood assuming continuous time and no ties is given by

\begin{equation}
    L(\Beta;\x) = \prod_{i=1}^k\frac{e^{\z_i(t)\Beta}}{\sum_{j\in\R_i}e^{\z_j(t)\Beta}}.
\end{equation}

By similar substitution of $\z(t)$ for $\z$, the methods and approximations for dealing with discrete and tied-time data can also be used for time-dependent covariates. Note that the form of $\z(t)$ must be determined which brings about problems in application; see Fisher and Lin \fcite{fisher-lin-99} for a more in depth discussion on time-varying covariates and the precautions that must be taken.

It is worth noting, as mentioned by Therneau et al \fcite{therneau-et-al-13}, if the effect of the time dependent covariate is predictable and \emph{linear}, it can be disregarded in the likelihood. Consider the partial likelihood of a single event with a single time-dependent covariate age, $a$, which linearly increases the risk:

\begin{equation}
    \frac{e^{a_{(i)}\beta}}{\sum_{j\in\R_i}e^{a_j\beta}}=\frac{e^{(a_{(i)}+t_({i}))\beta}}{\sum_{j\in\R_i}e^{(a_j+t_{(i)})\beta}}
\end{equation}

It is clear that the time-dependent and time-independent age are equivalent as $e^{t_{(i)}\beta}$ cancels on the RHS of the equation.

\subsection{Time-Varying Coefficients}

An extension to time-dependent coefficients is in direct opposition of the proportional hazards. Time-dependent coefficients can be considered, but the model will no longer be a proportional hazards model. There has been a relatively small amount of research done to extend specifically the cox model to non-proportional hazards as normally another model is more suitable in this case, e.g. one of the other hazard models suggested in the introduction. 

As suggested by Therneau et al \fcite{therneau-et-al-13}, one of the simplest forms that time-dependent coefficient can take is a step function, i.e. the coefficient takes different constant values over different time intervals. One method in dealing with this is to stratify the data (see section \ref{stratification}) into groups dependent on the time interval in which their event or censoring is observed.

Some continuous time-dependent coefficients can also be accommodated rather cheekily if the dependence is linear. Therneau et al \fcite{therneau-et-al-13} discussed how simple functional forms of the coeffcient function such as $\Beta(t)=a+b\log(t)$ have been used to trick software packages into thinking it is in fact the covariate that is time-dependent. Take for example 

$$\z_i\Beta_i(t)=a\z_i+ b\log(t)\z_i=a\z_i+by,$$

where $y=\log(t)\z_i$ is a special time-dependent covariate which was shown to be a simpler extension above. The intercept $a\z_i$, provided $z_i$ is time-invariant, will be absorbed by the baseline hazard in fitting the model, thus leaving a time-dependent covariate and a constant coefficient.

For further discussion on time-dependent coefficients, both step and continuous functions, see Zhang et al \fcite{zhang-18}.

\subsection{Counting Processes}

Andersen and Gill \fcite{andersen-gill-82} "discuss how this model can be extended to a model where covariate processes have a proportional effect on the intensity process of a multivariate counting process. This permits a statistical regression analysis of the intensity of a recurrent event allowing for complicated censoring patterns and time dependent covariates." I will not discuss the workings of the paper here, but it shows that the Cox model can be extended to indeed work with time-dependent covariates, time-dependent stratification and multiple events per subject. 

\newpage
\part{Variable Selection}

When considering the problem of variable selection, it is important to look at both accuracy of prediction and model interpretability: finding the best model will come down to balancing goodness of fit vs parsimony. The selected model should accurately predict future results based on the data already available, but it is also desirable that the model not be overly complex and thus does not make sense with real world data. This is particularly important with a view to the original purpose of the proportional hazards model, that being to estimate the effect of covariates irrespective of the arbitrary underlying distribution. One last feature that is definitely worth mentioning is computational costing. Even with the advances of modern computing which reduces the impact of the efficiency of the computation, with very large datasets (such as are available in cancer epidemiological studies) small optimisations can sway our propensity to choose one method over another.

There are very many (extremely good) methods for model selection when considering the simpler problem of linear regression in continuous time. However, not all of these work well for survival analysis with tied event times and censoring.

Most of the methods used today are generalizations of methods which were used for model selection for linear regression; an application to survival (time-to-event) data with tied times and censoring is not always obvious.

There are multiple schools of thought when considering a statistical approach to the problem. I will discuss the development of techniques from both a Bayesian and a frequentist standpoint, as well as how they are related, how they have been combined and some of the key features of each.

The problem of model selection can very often be decomposed into a problem of parameter selection (those within the model) and hyper-parameter estimation. I would refer the reader to \cite{bayesian-frequentist} for a more in-depth conversation about this distinction as well as discussion about the Bayesian / frequentist divide.

\section{Schools of Statistical Thought}

The two primary ways of approaching the problem of model selection are Bayesian and frequentist. 

\section{The Likelihood Function}



\section{Penalized Likelihood Methods}
\section{Subset Selection}
\section{Model Comparison} 
\subsection{Likelihood-Ratio Test}
\subsection{Wald Test}
\subsection{Lagrange Multiplier (Score) Test}
\part{Beyond the Model}
\section{Extensions}
\section{Alternative Models}
\part{Example}
\section{Application of Model Selection}